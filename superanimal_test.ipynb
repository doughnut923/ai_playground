{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a8ec08-619b-4beb-bf5a-39ed60066c59",
   "metadata": {},
   "source": [
    "# DeepLabCut - SuperAnimal Model Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d553e511-cc9d-46eb-a24d-e3428b8cd1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc13...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e70c85d4-11aa-49b8-8140-0741e21470e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\videos\"\n",
      "Created \"C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\labeled-data\"\n",
      "Created \"C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\training-datasets\"\n",
      "Created \"C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\dlc-models\"\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Created the symlink of C:\\Users\\jonas\\Github Repo\\AI\\test_video\\short_dolphin.mp4 to C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\videos\\short_dolphin.mp4\n",
      "C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\videos\\short_dolphin.mp4\n",
      "Generated \"C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\config.yaml\"\n",
      "\n",
      "A new project with name Test_Dolphin-Jonas-2026-01-29 is created at C:\\Users\\jonas\\Github Repo\\AI and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "config_path = deeplabcut.create_new_project(\n",
    "    \"Test_Dolphin\",\n",
    "    \"Jonas\",\n",
    "    [\"C:/Users/jonas/Github Repo/AI/test_video/short_dolphin.mp4\"],\n",
    "    working_directory=\"C:/Users/jonas/Github Repo/AI/\",\n",
    "    copy_videos=False,\n",
    "    multianimal=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21acd7fd-2a06-463e-a662-0a54a5ad237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jonas\\\\Github Repo\\\\AI\\\\Test_Dolphin-Jonas-2026-01-29\\\\config.yaml'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e1f17d3-e7c7-4a71-822f-5e595e66c7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 19.97  seconds.\n",
      "Extracting and downsampling... 599  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [00:02, 217.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[False]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.extract_frames(\n",
    "    config_path,\n",
    "    mode=\"automatic\",\n",
    "    algo=\"kmeans\",\n",
    "    crop=True,\n",
    "    userfeedback=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc2b177-56d9-47cd-9e1b-395884527d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1f9f06c-4bd3-470e-9439-043ca0ccd2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Jonas.\n",
      "Attention: C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\labeled-data\\dolphin does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21513db1-f13d-4541-9011-0eb7c8beb7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\labeled-data\\dolphin\\CollectedData_Jonas.h5  not found (perhaps not annotated).\n",
      "Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken...\n",
      "The following folders were found: ['C:\\\\Users\\\\jonas\\\\Github Repo\\\\AI\\\\Test_Dolphin-Jonas-2026-01-29\\\\labeled-data\\\\short_dolphin']\n",
      "Utilizing the following graph: [[0, 1], [0, 2], [1, 2]]\n",
      "Creating training data for: Shuffle: 1 TrainFraction:  0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 4756.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "116f4a31-9169-4a16-be4c-2b30a6ebb6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with configuration:\n",
      "data:\n",
      "  bbox_margin: 20\n",
      "  colormode: RGB\n",
      "  inference:\n",
      "    normalize_images: True\n",
      "  train:\n",
      "    affine:\n",
      "      p: 0.5\n",
      "      rotation: 30\n",
      "      scaling: [0.5, 1.25]\n",
      "      translation: 0\n",
      "    crop_sampling:\n",
      "      width: 448\n",
      "      height: 448\n",
      "      max_shift: 0.1\n",
      "      method: hybrid\n",
      "    gaussian_noise: 12.75\n",
      "    motion_blur: True\n",
      "    normalize_images: True\n",
      "device: auto\n",
      "inference:\n",
      "  multithreading:\n",
      "    enabled: True\n",
      "    queue_length: 4\n",
      "    timeout: 30.0\n",
      "  compile:\n",
      "    enabled: False\n",
      "    backend: inductor\n",
      "  autocast:\n",
      "    enabled: False\n",
      "metadata:\n",
      "  project_path: C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\n",
      "  pose_config_path: C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\dlc-models-pytorch\\iteration-0\\Test_DolphinJan29-trainset95shuffle1\\train\\pytorch_config.yaml\n",
      "  bodyparts: ['bodypart1', 'bodypart2', 'bodypart3']\n",
      "  unique_bodyparts: []\n",
      "  individuals: ['individual1', 'individual2']\n",
      "  with_identity: False\n",
      "method: bu\n",
      "model:\n",
      "  backbone:\n",
      "    type: ResNet\n",
      "    model_name: resnet50_gn\n",
      "    output_stride: 16\n",
      "    freeze_bn_stats: False\n",
      "    freeze_bn_weights: False\n",
      "  backbone_output_channels: 2048\n",
      "  heads:\n",
      "    bodypart:\n",
      "      type: DLCRNetHead\n",
      "      predictor:\n",
      "        type: PartAffinityFieldPredictor\n",
      "        num_animals: 2\n",
      "        num_multibodyparts: 3\n",
      "        num_uniquebodyparts: 0\n",
      "        nms_radius: 5\n",
      "        sigma: 1.0\n",
      "        locref_stdev: 7.2801\n",
      "        min_affinity: 0.05\n",
      "        graph: [[0, 1], [0, 2], [1, 2]]\n",
      "        edges_to_keep: [0, 1, 2]\n",
      "        apply_sigmoid: True\n",
      "        clip_scores: False\n",
      "      target_generator:\n",
      "        type: SequentialGenerator\n",
      "        generators: [{'type': 'HeatmapPlateauGenerator', 'num_heatmaps': 3, 'pos_dist_thresh': 17, 'heatmap_mode': 'KEYPOINT', 'gradient_masking': False, 'generate_locref': True, 'locref_std': 7.2801}, {'type': 'PartAffinityFieldGenerator', 'graph': [[0, 1], [0, 2], [1, 2]], 'width': 20}]\n",
      "      criterion:\n",
      "        heatmap:\n",
      "          type: WeightedBCECriterion\n",
      "          weight: 1.0\n",
      "        locref:\n",
      "          type: WeightedHuberCriterion\n",
      "          weight: 0.05\n",
      "        paf:\n",
      "          type: WeightedHuberCriterion\n",
      "          weight: 0.1\n",
      "      heatmap_config:\n",
      "        channels: [2048, 3]\n",
      "        kernel_size: [3]\n",
      "        strides: [2]\n",
      "      locref_config:\n",
      "        channels: [2048, 6]\n",
      "        kernel_size: [3]\n",
      "        strides: [2]\n",
      "      paf_config:\n",
      "        channels: [2048, 6]\n",
      "        kernel_size: [3]\n",
      "        strides: [2]\n",
      "      num_stages: 5\n",
      "net_type: resnet_50\n",
      "runner:\n",
      "  type: PoseTrainingRunner\n",
      "  gpus: None\n",
      "  key_metric: test.mAP\n",
      "  key_metric_asc: True\n",
      "  eval_interval: 10\n",
      "  optimizer:\n",
      "    type: AdamW\n",
      "    params:\n",
      "      lr: 0.0005\n",
      "  scheduler:\n",
      "    type: LRListScheduler\n",
      "    params:\n",
      "      lr_list: [[0.0001], [1e-05]]\n",
      "      milestones: [90, 120]\n",
      "  snapshots:\n",
      "    max_snapshots: 5\n",
      "    save_epochs: 25\n",
      "    save_optimizer_state: False\n",
      "train_settings:\n",
      "  batch_size: 8\n",
      "  dataloader_workers: 0\n",
      "  dataloader_pin_memory: False\n",
      "  display_iters: 500\n",
      "  epochs: 200\n",
      "  seed: 42\n",
      "Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)\n",
      "HTTP Request: HEAD https://huggingface.co/timm/resnet50_gn.a1h_in1k/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "[timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Data Transforms:\n",
      "  Training:   Compose([\n",
      "  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n",
      "  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),\n",
      "  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),\n",
      "  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),\n",
      "  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "  Validation: Compose([\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "Using 19 images and 1 for testing\n",
      "\n",
      "Starting pose model training...\n",
      "--------------------------------------------------\n",
      "Epoch 1/200 (lr=0.0005), train loss 0.20592, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 2/200 (lr=0.0005), train loss 0.06942, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 3/200 (lr=0.0005), train loss 0.03901, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 4/200 (lr=0.0005), train loss 0.02911, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 5/200 (lr=0.0005), train loss 0.02537, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 6/200 (lr=0.0005), train loss 0.02010, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 7/200 (lr=0.0005), train loss 0.02070, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 8/200 (lr=0.0005), train loss 0.02159, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 9/200 (lr=0.0005), train loss 0.02027, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 10 done, starting evaluation\n",
      "Epoch 10/200 (lr=0.0005), train loss 0.02017, valid loss 0.01458, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:          73.65\n",
      "  metrics/test.rmse_pcutoff:    nan\n",
      "  metrics/test.mAP:           10.10\n",
      "  metrics/test.mAR:           20.00\n",
      "Epoch 11/200 (lr=0.0005), train loss 0.01972, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 12/200 (lr=0.0005), train loss 0.01656, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 13/200 (lr=0.0005), train loss 0.01619, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 14/200 (lr=0.0005), train loss 0.01669, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 15/200 (lr=0.0005), train loss 0.01538, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 16/200 (lr=0.0005), train loss 0.01428, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 17/200 (lr=0.0005), train loss 0.01586, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 18/200 (lr=0.0005), train loss 0.01484, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 19/200 (lr=0.0005), train loss 0.01459, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 20 done, starting evaluation\n",
      "Epoch 20/200 (lr=0.0005), train loss 0.01441, valid loss 0.01148, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:          29.93\n",
      "  metrics/test.rmse_pcutoff:  10.76\n",
      "  metrics/test.mAP:           37.57\n",
      "  metrics/test.mAR:           45.00\n",
      "Epoch 21/200 (lr=0.0005), train loss 0.01324, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 22/200 (lr=0.0005), train loss 0.01283, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 23/200 (lr=0.0005), train loss 0.01286, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 24/200 (lr=0.0005), train loss 0.01113, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 25/200 (lr=0.0005), train loss 0.01146, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 26/200 (lr=0.0005), train loss 0.01100, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 27/200 (lr=0.0005), train loss 0.01026, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 28/200 (lr=0.0005), train loss 0.00994, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 29/200 (lr=0.0005), train loss 0.00981, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 30 done, starting evaluation\n",
      "Epoch 30/200 (lr=0.0005), train loss 0.00929, valid loss 0.00709, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.58\n",
      "  metrics/test.rmse_pcutoff:   4.12\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 31/200 (lr=0.0005), train loss 0.00805, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 32/200 (lr=0.0005), train loss 0.00816, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 33/200 (lr=0.0005), train loss 0.00678, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 34/200 (lr=0.0005), train loss 0.00782, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 35/200 (lr=0.0005), train loss 0.00693, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 36/200 (lr=0.0005), train loss 0.00650, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 37/200 (lr=0.0005), train loss 0.00604, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 38/200 (lr=0.0005), train loss 0.00538, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 39/200 (lr=0.0005), train loss 0.00541, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 40 done, starting evaluation\n",
      "Epoch 40/200 (lr=0.0005), train loss 0.00527, valid loss 0.00432, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.48\n",
      "  metrics/test.rmse_pcutoff:   4.48\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 41/200 (lr=0.0005), train loss 0.00535, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 42/200 (lr=0.0005), train loss 0.00565, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 43/200 (lr=0.0005), train loss 0.00440, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 44/200 (lr=0.0005), train loss 0.00504, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 45/200 (lr=0.0005), train loss 0.00400, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 46/200 (lr=0.0005), train loss 0.00355, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 47/200 (lr=0.0005), train loss 0.00437, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 48/200 (lr=0.0005), train loss 0.00524, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 49/200 (lr=0.0005), train loss 0.00419, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 50 done, starting evaluation\n",
      "Epoch 50/200 (lr=0.0005), train loss 0.00412, valid loss 0.00348, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           5.36\n",
      "  metrics/test.rmse_pcutoff:   5.36\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 51/200 (lr=0.0005), train loss 0.00346, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 52/200 (lr=0.0005), train loss 0.00394, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 53/200 (lr=0.0005), train loss 0.00410, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 54/200 (lr=0.0005), train loss 0.00306, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 55/200 (lr=0.0005), train loss 0.00404, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 56/200 (lr=0.0005), train loss 0.00384, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 57/200 (lr=0.0005), train loss 0.00441, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 58/200 (lr=0.0005), train loss 0.00313, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 59/200 (lr=0.0005), train loss 0.00338, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 60 done, starting evaluation\n",
      "Epoch 60/200 (lr=0.0005), train loss 0.00335, valid loss 0.00274, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           5.82\n",
      "  metrics/test.rmse_pcutoff:   5.82\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 61/200 (lr=0.0005), train loss 0.00310, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 62/200 (lr=0.0005), train loss 0.00276, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 63/200 (lr=0.0005), train loss 0.00374, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 64/200 (lr=0.0005), train loss 0.00267, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 65/200 (lr=0.0005), train loss 0.00333, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 66/200 (lr=0.0005), train loss 0.00321, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 67/200 (lr=0.0005), train loss 0.00278, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 68/200 (lr=0.0005), train loss 0.00295, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 69/200 (lr=0.0005), train loss 0.00294, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 70 done, starting evaluation\n",
      "Epoch 70/200 (lr=0.0005), train loss 0.00247, valid loss 0.00222, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.34\n",
      "  metrics/test.rmse_pcutoff:   3.34\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 71/200 (lr=0.0005), train loss 0.00282, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 72/200 (lr=0.0005), train loss 0.00303, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 73/200 (lr=0.0005), train loss 0.00256, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 74/200 (lr=0.0005), train loss 0.00244, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 75/200 (lr=0.0005), train loss 0.00228, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 76/200 (lr=0.0005), train loss 0.00278, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 77/200 (lr=0.0005), train loss 0.00289, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 78/200 (lr=0.0005), train loss 0.00225, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 79/200 (lr=0.0005), train loss 0.00264, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 80 done, starting evaluation\n",
      "Epoch 80/200 (lr=0.0005), train loss 0.00274, valid loss 0.00223, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.06\n",
      "  metrics/test.rmse_pcutoff:   4.06\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 81/200 (lr=0.0005), train loss 0.00208, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 82/200 (lr=0.0005), train loss 0.00232, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 83/200 (lr=0.0005), train loss 0.00224, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 84/200 (lr=0.0005), train loss 0.00237, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 85/200 (lr=0.0005), train loss 0.00202, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 86/200 (lr=0.0005), train loss 0.00211, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 87/200 (lr=0.0005), train loss 0.00225, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 88/200 (lr=0.0005), train loss 0.00228, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 89/200 (lr=0.0005), train loss 0.00334, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 90 done, starting evaluation\n",
      "Epoch 90/200 (lr=0.0001), train loss 0.00333, valid loss 0.00204, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           2.82\n",
      "  metrics/test.rmse_pcutoff:   2.82\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 91/200 (lr=0.0001), train loss 0.00240, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 92/200 (lr=0.0001), train loss 0.00205, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 93/200 (lr=0.0001), train loss 0.00204, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 94/200 (lr=0.0001), train loss 0.00177, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 95/200 (lr=0.0001), train loss 0.00209, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 96/200 (lr=0.0001), train loss 0.00212, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 97/200 (lr=0.0001), train loss 0.00195, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 98/200 (lr=0.0001), train loss 0.00156, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 99/200 (lr=0.0001), train loss 0.00179, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 100 done, starting evaluation\n",
      "Epoch 100/200 (lr=0.0001), train loss 0.00173, valid loss 0.00177, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.96\n",
      "  metrics/test.rmse_pcutoff:   3.96\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 101/200 (lr=0.0001), train loss 0.00167, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 102/200 (lr=0.0001), train loss 0.00232, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 103/200 (lr=0.0001), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 104/200 (lr=0.0001), train loss 0.00232, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 105/200 (lr=0.0001), train loss 0.00161, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 106/200 (lr=0.0001), train loss 0.00153, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 107/200 (lr=0.0001), train loss 0.00164, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 108/200 (lr=0.0001), train loss 0.00187, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 109/200 (lr=0.0001), train loss 0.00163, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 110 done, starting evaluation\n",
      "Epoch 110/200 (lr=0.0001), train loss 0.00168, valid loss 0.00157, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.68\n",
      "  metrics/test.rmse_pcutoff:   3.68\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 111/200 (lr=0.0001), train loss 0.00207, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 112/200 (lr=0.0001), train loss 0.00128, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 113/200 (lr=0.0001), train loss 0.00161, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 114/200 (lr=0.0001), train loss 0.00188, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 115/200 (lr=0.0001), train loss 0.00203, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 116/200 (lr=0.0001), train loss 0.00169, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 117/200 (lr=0.0001), train loss 0.00256, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 118/200 (lr=0.0001), train loss 0.00162, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 119/200 (lr=0.0001), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 120 done, starting evaluation\n",
      "Epoch 120/200 (lr=1e-05), train loss 0.00161, valid loss 0.00160, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.97\n",
      "  metrics/test.rmse_pcutoff:   3.97\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 121/200 (lr=1e-05), train loss 0.00208, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 122/200 (lr=1e-05), train loss 0.00137, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 123/200 (lr=1e-05), train loss 0.00209, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 124/200 (lr=1e-05), train loss 0.00145, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 125/200 (lr=1e-05), train loss 0.00174, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 126/200 (lr=1e-05), train loss 0.00162, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 127/200 (lr=1e-05), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 128/200 (lr=1e-05), train loss 0.00149, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 129/200 (lr=1e-05), train loss 0.00192, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 130 done, starting evaluation\n",
      "Epoch 130/200 (lr=1e-05), train loss 0.00184, valid loss 0.00178, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.91\n",
      "  metrics/test.rmse_pcutoff:   3.91\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 131/200 (lr=1e-05), train loss 0.00169, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 132/200 (lr=1e-05), train loss 0.00165, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 133/200 (lr=1e-05), train loss 0.00189, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 134/200 (lr=1e-05), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 135/200 (lr=1e-05), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 136/200 (lr=1e-05), train loss 0.00158, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 137/200 (lr=1e-05), train loss 0.00140, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 138/200 (lr=1e-05), train loss 0.00206, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 139/200 (lr=1e-05), train loss 0.00167, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 140 done, starting evaluation\n",
      "Epoch 140/200 (lr=1e-05), train loss 0.00168, valid loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.87\n",
      "  metrics/test.rmse_pcutoff:   3.87\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 141/200 (lr=1e-05), train loss 0.00222, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 142/200 (lr=1e-05), train loss 0.00166, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 143/200 (lr=1e-05), train loss 0.00168, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 144/200 (lr=1e-05), train loss 0.00179, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 145/200 (lr=1e-05), train loss 0.00127, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 146/200 (lr=1e-05), train loss 0.00176, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 147/200 (lr=1e-05), train loss 0.00195, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 148/200 (lr=1e-05), train loss 0.00159, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 149/200 (lr=1e-05), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 150 done, starting evaluation\n",
      "Epoch 150/200 (lr=1e-05), train loss 0.00175, valid loss 0.00179, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.89\n",
      "  metrics/test.rmse_pcutoff:   3.89\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 151/200 (lr=1e-05), train loss 0.00144, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 152/200 (lr=1e-05), train loss 0.00168, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 153/200 (lr=1e-05), train loss 0.00150, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 154/200 (lr=1e-05), train loss 0.00163, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 155/200 (lr=1e-05), train loss 0.00173, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 156/200 (lr=1e-05), train loss 0.00206, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 157/200 (lr=1e-05), train loss 0.00166, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 158/200 (lr=1e-05), train loss 0.00213, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 159/200 (lr=1e-05), train loss 0.00154, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 160 done, starting evaluation\n",
      "Epoch 160/200 (lr=1e-05), train loss 0.00158, valid loss 0.00170, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.16\n",
      "  metrics/test.rmse_pcutoff:   4.16\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 161/200 (lr=1e-05), train loss 0.00178, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 162/200 (lr=1e-05), train loss 0.00226, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 163/200 (lr=1e-05), train loss 0.00168, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 164/200 (lr=1e-05), train loss 0.00165, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 165/200 (lr=1e-05), train loss 0.00151, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 166/200 (lr=1e-05), train loss 0.00236, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 167/200 (lr=1e-05), train loss 0.00146, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 168/200 (lr=1e-05), train loss 0.00175, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 169/200 (lr=1e-05), train loss 0.00213, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 170 done, starting evaluation\n",
      "Epoch 170/200 (lr=1e-05), train loss 0.00177, valid loss 0.00185, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.28\n",
      "  metrics/test.rmse_pcutoff:   4.28\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 171/200 (lr=1e-05), train loss 0.00199, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 172/200 (lr=1e-05), train loss 0.00185, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 173/200 (lr=1e-05), train loss 0.00178, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 174/200 (lr=1e-05), train loss 0.00171, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 175/200 (lr=1e-05), train loss 0.00184, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 176/200 (lr=1e-05), train loss 0.00154, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 177/200 (lr=1e-05), train loss 0.00135, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 178/200 (lr=1e-05), train loss 0.00157, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 179/200 (lr=1e-05), train loss 0.00165, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 180 done, starting evaluation\n",
      "Epoch 180/200 (lr=1e-05), train loss 0.00133, valid loss 0.00178, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.22\n",
      "  metrics/test.rmse_pcutoff:   4.22\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 181/200 (lr=1e-05), train loss 0.00167, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 182/200 (lr=1e-05), train loss 0.00189, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 183/200 (lr=1e-05), train loss 0.00186, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 184/200 (lr=1e-05), train loss 0.00147, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 185/200 (lr=1e-05), train loss 0.00148, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 186/200 (lr=1e-05), train loss 0.00160, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 187/200 (lr=1e-05), train loss 0.00237, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 188/200 (lr=1e-05), train loss 0.00181, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 189/200 (lr=1e-05), train loss 0.00141, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 190 done, starting evaluation\n",
      "Epoch 190/200 (lr=1e-05), train loss 0.00156, valid loss 0.00176, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           4.15\n",
      "  metrics/test.rmse_pcutoff:   4.15\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n",
      "Epoch 191/200 (lr=1e-05), train loss 0.00175, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 192/200 (lr=1e-05), train loss 0.00184, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 193/200 (lr=1e-05), train loss 0.00172, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 194/200 (lr=1e-05), train loss 0.00183, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 195/200 (lr=1e-05), train loss 0.00184, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 196/200 (lr=1e-05), train loss 0.00172, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 197/200 (lr=1e-05), train loss 0.00226, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 198/200 (lr=1e-05), train loss 0.00209, GPU: 4006.0/8187.5 MiB\n",
      "Epoch 199/200 (lr=1e-05), train loss 0.00163, GPU: 4006.0/8187.5 MiB\n",
      "Training for epoch 200 done, starting evaluation\n",
      "Epoch 200/200 (lr=1e-05), train loss 0.00162, valid loss 0.00162, GPU: 4006.0/8187.5 MiB\n",
      "Model performance:\n",
      "  metrics/test.rmse:           3.97\n",
      "  metrics/test.rmse_pcutoff:   3.97\n",
      "  metrics/test.mAP:          100.00\n",
      "  metrics/test.mAR:          100.00\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a429867a-3cbd-4993-9ee0-5d169c4aec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation scorer: DLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00, 13.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 30.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results file: DLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30-results.csv\n",
      "Evaluation results for DLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30-results.csv (pcutoff: 0.6):\n",
      "train rmse             21.45\n",
      "train rmse_pcutoff     21.17\n",
      "train mAP              63.11\n",
      "train mAR              69.47\n",
      "test rmse               4.58\n",
      "test rmse_pcutoff       4.12\n",
      "test mAP              100.00\n",
      "test mAR              100.00\n",
      "Name: (0.95, 1, 30, -1, 0.6), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(config_path, Shuffles=[1], plotting=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57f5046d-df71-48b5-91f7-72fd8c97d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing videos with C:\\Users\\jonas\\Github Repo\\AI\\Test_Dolphin-Jonas-2026-01-29\\dlc-models-pytorch\\iteration-0\\Test_DolphinJan29-trainset95shuffle1\\train\\snapshot-best-030.pt\n",
      "Using scorer: DLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30\n",
      "Starting to analyze C:\\Users\\jonas\\Github Repo\\AI\\test_video\\longer_dolphin.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    716\n",
      "  Duration of video [s]:  23.87\n",
      "  fps:                    30.0\n",
      "  resolution:             w=1280, h=720\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 716/716 [00:32<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...  C:\\Users\\jonas\\Github Repo\\AI\\test_video\\longer_dolphin.mp4\n",
      "Loading From C:\\Users\\jonas\\Github Repo\\AI\\test_video\\longer_dolphinDLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 716/716 [00:07<00:00, 94.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tracklets were created (i.e., under the hood deeplabcut.convert_detections2tracklets was run). Now you can 'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'.\n",
      "Processing...  C:\\Users\\jonas\\Github Repo\\AI\\test_video\\longer_dolphin.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 6335.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DLC_Resnet50_Test_DolphinJan29shuffle1_snapshot_best-30'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(\n",
    "    config_path, [\"C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4\"],\n",
    "    save_as_csv=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1ff8952-49cf-472d-b344-ee0877beb31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process video: C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4\n",
      "Loading C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4 and data.\n",
      "Duration of video [s]: 23.87, recorded with 30.0 fps!\n",
      "Overall # of frames: 716 with cropped frame dimensions: 1280 720\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 704/704 [00:04<00:00, 169.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(\n",
    "    config_path,\n",
    "    [\"C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4\"],\n",
    "    save_frames = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac290a89-05bb-44ac-81ef-04adeb0ea813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4 and data.\n",
      "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.plot_trajectories(config_path, \"C:/Users/jonas/Github Repo/AI/test_video/longer_dolphin.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce3828b2-e91d-447b-9d3f-70bf94a5a192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animaltokenpose_base',\n",
       " 'cspnext_m',\n",
       " 'cspnext_s',\n",
       " 'cspnext_x',\n",
       " 'ctd_coam_w32',\n",
       " 'ctd_coam_w48',\n",
       " 'ctd_coam_w48_human',\n",
       " 'ctd_prenet_hrnet_w32',\n",
       " 'ctd_prenet_hrnet_w48',\n",
       " 'ctd_prenet_rtmpose_m',\n",
       " 'ctd_prenet_rtmpose_s',\n",
       " 'ctd_prenet_rtmpose_x',\n",
       " 'ctd_prenet_rtmpose_x_human',\n",
       " 'dekr_w18',\n",
       " 'dekr_w32',\n",
       " 'dekr_w48',\n",
       " 'dlcrnet_stride16_ms5',\n",
       " 'dlcrnet_stride32_ms5',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w48',\n",
       " 'resnet_101',\n",
       " 'resnet_50',\n",
       " 'rtmpose_m',\n",
       " 'rtmpose_s',\n",
       " 'rtmpose_x',\n",
       " 'top_down_cspnext_m',\n",
       " 'top_down_cspnext_s',\n",
       " 'top_down_cspnext_x',\n",
       " 'top_down_hrnet_w18',\n",
       " 'top_down_hrnet_w32',\n",
       " 'top_down_hrnet_w48',\n",
       " 'top_down_resnet_101',\n",
       " 'top_down_resnet_50']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.pose_estimation_pytorch.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71918c-57e5-4db8-90b7-4b485f7fb47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
